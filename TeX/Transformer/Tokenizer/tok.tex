\documentclass{article}
\usepackage{enumitem}

\title{MIDI Transformer Tokenization}
\author{Jonas Veit, NAMEN EINFÃœGEN }
\date{\today}

\begin{document}
    \maketitle
    \section*{Current workflow}
    \begin{enumerate}
        \item Specify the MIDI data
        \item Select a suitable Tokenizer. Right now we use REMI
        \item train the tokenizer with BPE
        \item save the tokenizer and vocabulary as json
        \item generate training, validation and test sets via generate\_train\_val\_test\_sets.py we already augment data witihin this process if we want to.
        \item Our GoePT expects integers as tokens and np.memmap expects binary files. We use np.to\_file in order to create the right binaries
    \end{enumerate}
    
    \subsection*{Different tokenizers}
    \begin{itemize}
        \item For simple melodies: REMI or MelodyTokenizer.
        \item For more complex, polyphonic data: Consider PolyphonyTokenizer or NoteTokenizer.
        \item For structured MIDI data: Use Structured or TSD.
        \item For music generation: REMI or MIDI-Like might work well.
    \end{itemize}

    \subsection*{Things to keep in mind (Open Problems)}
    \begin{itemize}
        \item We need to specify the sequence length
        \item We need to specify the embedding dimension
        \item We need to specfy the vocab\_size
        \item When specifying a $seq_len > 32$ the data generation process fails. I have to investigate this. This problem is related to the function: miditok.split\_files\_for\_training
        \item When we want to regenerate the train, val and test sets we need to manually delete the dataset\_ folders! I will write some code for that soon.
        \item I think self.context\_length from GoePT can be larger than the actualy seq\_len specified within the tokenizer. What should we do here?
    \end{itemize}


\end{document}
