@misc{shaw2018selfattentionrelativepositionrepresentations,
      title={Self-Attention with Relative Position Representations}, 
      author={Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
      year={2018},
      eprint={1803.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1803.02155}, 
}


@article{DBLP:journals/corr/abs-1809-04281,
  author       = {Cheng{-}Zhi Anna Huang and
                  Ashish Vaswani and
                  Jakob Uszkoreit and
                  Noam Shazeer and
                  Curtis Hawthorne and
                  Andrew M. Dai and
                  Matthew D. Hoffman and
                  Douglas Eck},
  title        = {An Improved Relative Self-Attention Mechanism for Transformer with
                  Application to Music Generation},
  journal      = {CoRR},
  volume       = {abs/1809.04281},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.04281},
  eprinttype    = {arXiv},
  eprint       = {1809.04281},
  timestamp    = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-04281.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@online{TopPWebsite,
    author = {Jackson Stokes},
    title = {A guide to language model sampling in AllenNLP},
    year = {2020},
    url = {https://medium.com/ai2-blog/a-guide-to-language-model-sampling-in-allennlp-3b1239274bc3},
    note = {Accessed: 2025-02-27}
}

@online{GenreDistr,
    author = {},
    title = {Lakh Clean Midi Dataset Analysis},
    year = {2024},
    url = {https://lakhcleananalysis.sourceforge.io/},
    note = {Accessed: 2025-02-27}
}

@misc{Raffel2016LearningBasedMF,
  title={Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching},
  author={Colin Raffel},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:63439223}
}

@misc{holtzman2020curiouscaseneuraltext,
      title={The Curious Case of Neural Text Degeneration}, 
      author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
      year={2020},
      eprint={1904.09751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09751}, 
}This website uses cookies

We occasionally run membership recruitment campaigns on social media channels and use cookies to track post-clicks. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. Use the check boxes below to choose the types of cookies you consent to have stored on your device.
Use necessary cookies only Allow selected cookies Allow all cookies
Necessary
Preferences
Statistics
Marketing
Show details
skip to main content

    ACM Digital Library home
    ACM Association for Computing Machinery corporate logo

    Browse About
        Sign in Register 

    Journals
    Magazines
    Proceedings
    Books
    SIGs
    Conferences
    People

Search ACM Digital Library
Advanced Search

    Conference
    Proceedings
    Upcoming Events
    Authors
    Affiliations
    Award Winners

    HomeConferencesMMProceedingsMM '20Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions

research-article
Share on

Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions
Authors: Yu-Siang Huang, Yi-Hsuan YangAuthors Info & Claims
MM '20: Proceedings of the 28th ACM International Conference on Multimedia
Pages 1180 - 1188
https://doi.org/10.1145/3394171.3413671
Published: 12 October 2020 Publication History
Get Access
MM '20: Proceedings of the 28th ACM International Conference on Multimedia
Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions
Pages 1180 - 1188
Previous
Next

    Abstract
    Supplementary Material
    References
    Cited By
    Index Terms
    Recommendations
    Comments

ACM Digital Library

    Information & Contributors
    Bibliometrics & Citations
    Get Access
    References45

    Share

Abstract
A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models.
Supplementary Material
MP4 File (3394171.3413671.mp4)
We?ve developed Pop Music Transformer, a deep learning model that can generate pieces of expressive Pop piano music of several minutes. Unlike existing models for music composition, our model learns to compose music over a metrical structure defined in terms of bars, beats, and sub-beats. As a result, our model can generate music with more salient and consistent rhythmic structure.

    Download
    11.60 MB

References
[1]
Emmanouil Benetos, Simon Dixon, Zhiyao Duan, and Sebastian Ewert. 2018. Automatic music transcription: An overview. IEEE Signal Processing Magazine 36, 1 (2018), 20--30.
Crossref
Google Scholar
[2]
Sebastian Böck, Filip Korzeniowski, Jan Schlüter, Florian Krebs, and Gerhard Widmer. 2016. Madmom: A new python audio and music signal processing library. In Proc. ACM Multimedia. 1174--1178.
Digital Library
Google Scholar
[3]
Sebastian Böck, Florian Krebs, and Gerhard Widmer. 2016. Joint beat and down- beat tracking with recurrent neural networks. In Proc. Int. Soc. Music Information Retrieval Conf. 255--261.
Google Scholar
[4]
Dmitry Bogdanov, J. Serrà, Nicolas Wack, and Perfecto Herrera. 2009. From low-level to high-level: Comparative study of music similarity measures. In Proc. IEEE International Symposium on Multimedia.
Digital Library
Google Scholar
Cited By
View all

    Zhang YZhou YLv XLi JLu HSu YYang H(2025)TARREAN: A Novel Transformer with a Gate Recurrent Unit for Stylized Music GenerationSensors10.3390/s2502038625:2(386)Online publication date: 10-Jan-2025
    https://doi.org/10.3390/s25020386
    Le DBigo LHerremans DKeller M(2025)Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: A SurveyACM Computing Surveys10.1145/371445757:7(1-40)Online publication date: 28-Jan-2025
    https://dl.acm.org/doi/10.1145/3714457
    Hachana RRasheed B(2025)Probe-Assisted Fine-Grained Control for Non-Differentiable Features in Symbolic Music GenerationIEEE Access10.1109/ACCESS.2025.354054313(28059-28070)Online publication date: 2025
    https://doi.org/10.1109/ACCESS.2025.3540543
    Show More Cited By

Index Terms

    Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions

        Applied computing

            Arts and humanities

                Sound and music computing

Recommendations

    Structure-Enhanced Pop Music Generation via Harmony-Aware Learning
    MM '22: Proceedings of the 30th ACM International Conference on Multimedia

    Pop music generation has always been an attractive topic for both musicians and scientists for a long time. However, automatically composing pop music with a satisfactory structure is still a challenging issue. In this paper, we propose to leverage ...
    Read More
    Video Background Music Generation with Controllable Music Transformer
    MM '21: Proceedings of the 29th ACM International Conference on Multimedia

    In this work, we address the task of video background music generation. Some previous works achieve effective music generation but are unable to generate melodious music specifically for a given video, and none of them considers the video-music rhythmic ...
    Read More
    Pop Music Generation: From Melody to Multi-style Arrangement
    Special Issue on KDD 2018, Regular Papers and Survey Paper

    Music plays an important role in our daily life. With the development of deep learning and modern generation techniques, researchers have done plenty of works on automatic music generation. However, due to the special requirements of both melody and ...
    Read More

Comments
Download PDF
View Table of Conten
Export Citations

    @inproceedings{10.1145/3394171.3413671,
    author = {Huang, Yu-Siang and Yang, Yi-Hsuan},
    title = {Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions},
    year = {2020},
    isbn = {9781450379885},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3394171.3413671},
    doi = {10.1145/3394171.3413671},
    abstract = {A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models.},
    booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
    pages = {1180–1188},
    numpages = {9},
    keywords = {automatic music composition, neural sequence model, transformer},
    location = {Seattle, WA, USA},
    series = {MM '20}
    }

      

Footer
Categories

    Journals
    Magazines
    Books
    Proceedings
    SIGs
    Conferences
    Collections
    People

About

    About ACM Digital Library
    ACM Digital Library Board
    Subscription Information
    Author Guidelines
    Using ACM Digital Library
    All Holdings within the ACM Digital Library
    ACM Computing Classification System
    Accessibility Statement

Join

    Join ACM
    Join SIGs
    Subscribe to Publications
    Institutions and Libraries

Connect

    Contact us via email
    ACM on Facebook
    ACM DL on X
    ACM on Linkedin
    Send Feedback
    Submit a Bug Report

The ACM Digital Library is published by the Association for Computing Machinery. Copyright © 2025 ACM, Inc.

    Terms of Usage Privacy Policy Code of Ethics 

ACM Digital Library home
ACM Association for Computing Machinery corporate logo


@inproceedings{huang_remi_2020,
    author = {Huang, Yu-Siang and Yang, Yi-Hsuan},
    title = {Pop Music Transformer: Beat-Based Modeling and Generation of Expressive Pop Piano Compositions},
    year = {2020},
    isbn = {9781450379885},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3394171.3413671},
    doi = {10.1145/3394171.3413671},
    booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
    pages = {1180–1188},
    numpages = {9},
    keywords = {transformer, neural sequence model, automatic music composition},
    location = {Seattle, WA, USA},
    series = {MM '20}
}


@inproceedings{miditok2021,
    title={{MidiTok}: A Python package for {MIDI} file tokenization},
    author={Fradet, Nathan and Briot, Jean-Pierre and Chhel, Fabien and El Fallah Seghrouchni, Amal and Gutowski, Nicolas},
    booktitle={Extended Abstracts for the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference},
    year={2021},
    url={https://archives.ismir.net/ismir2021/latebreaking/000005.pdf},
}