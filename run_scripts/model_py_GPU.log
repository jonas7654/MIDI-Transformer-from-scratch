Running Training (with CUPY)
context length: 128
epochs: 5
batch size: 16
eval_interval: 1
learning rate: 0.1
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: me322 (me322-university-of-heidelberg). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /csghome/hpdc04/Transformer_Code/run_scripts/wandb/run-20250114_194830-zgrhijx5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-butterfly-33
wandb: ⭐️ View project at https://wandb.ai/me322-university-of-heidelberg/MIDI-Transformer
wandb: 🚀 View run at https://wandb.ai/me322-university-of-heidelberg/MIDI-Transformer/runs/zgrhijx5
Starting epoch: 1
Current local training loss: 3.18398e-01
Current local training loss: 2.44801e-01
Current local training loss: 2.36941e-01
Current local training loss: 2.31971e-01
Current local training loss: 2.27093e-01
Current local training loss: 2.23316e-01
Current local training loss: 2.19991e-01
Current local training loss: 2.16531e-01
Current local training loss: 2.13173e-01
Current local training loss: 2.10377e-01
Current local training loss: 2.07191e-01
Current local training loss: 2.04471e-01
Current local training loss: 2.01918e-01
Current local training loss: 1.99095e-01
Current local training loss: 1.96719e-01
Current local training loss: 1.94312e-01
Current local training loss: 1.91606e-01
Current local training loss: 1.89155e-01
Current local training loss: 1.87237e-01
Current local training loss: 1.85287e-01
Current local training loss: 1.83240e-01
Current local training loss: 1.81611e-01
Current local training loss: 1.79729e-01
Current local training loss: 1.78053e-01
Current local training loss: 1.76166e-01
Current local training loss: 1.74517e-01
Current local training loss: 1.72662e-01
Current local training loss: 1.71379e-01
Current local training loss: 1.69705e-01
Current local training loss: 1.68023e-01
Current local training loss: 1.66497e-01
Current local training loss: 1.65158e-01
Starting epoch: 2
Current local training loss: 2.52587e-01
Current local training loss: 2.48931e-01
Current local training loss: 2.44783e-01
Current local training loss: 2.41623e-01
Current local training loss: 2.37979e-01
Current local training loss: 2.34529e-01
Current local training loss: 2.31165e-01
Current local training loss: 2.27838e-01
Current local training loss: 2.24702e-01
Current local training loss: 2.22097e-01
Current local training loss: 2.19155e-01
Current local training loss: 2.15904e-01
Current local training loss: 2.13079e-01
Current local training loss: 2.10077e-01
Current local training loss: 2.07222e-01
Current local training loss: 2.04777e-01
Current local training loss: 2.01914e-01
Current local training loss: 1.99151e-01
Current local training loss: 1.96903e-01
Current local training loss: 1.94847e-01
Current local training loss: 1.92861e-01
Current local training loss: 1.90518e-01
Current local training loss: 1.88450e-01
Current local training loss: 1.86259e-01
Current local training loss: 1.84325e-01
Current local training loss: 1.82772e-01
Current local training loss: 1.81046e-01
Current local training loss: 1.79274e-01
Current local training loss: 1.77604e-01
Current local training loss: 1.75762e-01
Current local training loss: 1.73783e-01
Current local training loss: 1.71880e-01
Starting epoch: 3
Current local training loss: 2.56998e-01
Current local training loss: 2.51864e-01
Current local training loss: 2.47959e-01
Current local training loss: 2.45027e-01
Current local training loss: 2.41678e-01
Current local training loss: 2.38344e-01
Current local training loss: 2.35054e-01
Current local training loss: 2.31729e-01
Current local training loss: 2.28306e-01
Current local training loss: 2.25111e-01
Current local training loss: 2.21718e-01
Current local training loss: 2.18620e-01
Current local training loss: 2.14554e-01
Current local training loss: 2.11778e-01
Current local training loss: 2.08536e-01
Current local training loss: 2.05428e-01
Current local training loss: 2.03276e-01
Current local training loss: 2.00883e-01
Current local training loss: 1.98549e-01
Current local training loss: 1.96575e-01
Current local training loss: 1.93751e-01
Current local training loss: 1.91679e-01
Current local training loss: 1.89430e-01
Current local training loss: 1.86804e-01
Current local training loss: 1.84600e-01
Current local training loss: 1.82195e-01
Current local training loss: 1.80246e-01
Current local training loss: 1.78531e-01
Current local training loss: 1.76808e-01
Current local training loss: 1.75163e-01
Current local training loss: 1.73505e-01
Current local training loss: 1.72087e-01
Starting epoch: 4
Current local training loss: 2.47168e-01
Current local training loss: 2.42736e-01
Current local training loss: 2.38756e-01
Current local training loss: 2.35295e-01
Current local training loss: 2.30844e-01
Current local training loss: 2.26396e-01
Current local training loss: 2.23069e-01
Current local training loss: 2.19673e-01
Current local training loss: 2.16348e-01
Current local training loss: 2.12725e-01
Current local training loss: 2.08541e-01
Current local training loss: 2.05169e-01
Current local training loss: 2.02528e-01
Current local training loss: 1.99863e-01
Current local training loss: 1.96875e-01
Current local training loss: 1.94151e-01
Current local training loss: 1.91959e-01
Current local training loss: 1.89388e-01
Current local training loss: 1.86523e-01
Current local training loss: 1.83940e-01
Current local training loss: 1.81374e-01
Current local training loss: 1.79204e-01
Current local training loss: 1.77346e-01
Current local training loss: 1.75533e-01
Current local training loss: 1.73899e-01
Current local training loss: 1.72191e-01
Current local training loss: 1.70386e-01
Current local training loss: 1.68385e-01
Current local training loss: 1.66787e-01
Current local training loss: 1.65511e-01
Current local training loss: 1.64437e-01
Current local training loss: 1.63273e-01
Starting epoch: 5
Current local training loss: 2.52819e-01
Current local training loss: 2.47565e-01
Current local training loss: 2.43885e-01
Current local training loss: 2.40636e-01
Current local training loss: 2.37071e-01
Current local training loss: 2.34297e-01
Current local training loss: 2.30791e-01
Current local training loss: 2.26978e-01
Current local training loss: 2.23161e-01
Current local training loss: 2.19157e-01
Current local training loss: 2.15183e-01
Current local training loss: 2.11111e-01
Current local training loss: 2.07247e-01
Current local training loss: 2.04483e-01
Current local training loss: 2.01783e-01
Current local training loss: 1.98890e-01
Current local training loss: 1.96453e-01
Current local training loss: 1.93916e-01
Current local training loss: 1.91416e-01
Current local training loss: 1.89508e-01
Current local training loss: 1.87432e-01
Current local training loss: 1.84918e-01
Current local training loss: 1.82272e-01
Current local training loss: 1.79804e-01
Current local training loss: 1.77598e-01
Current local training loss: 1.75555e-01
Current local training loss: 1.73475e-01
Current local training loss: 1.71551e-01
Current local training loss: 1.69826e-01
Current local training loss: 1.68223e-01
Current local training loss: 1.66869e-01
Current local training loss: 1.65177e-01
Starting epoch: 6
Current local training loss: 2.79447e-01
Current local training loss: 2.72623e-01
Current local training loss: 2.66009e-01
Current local training loss: 2.60082e-01
Current local training loss: 2.53714e-01
Current local training loss: 2.47686e-01
Current local training loss: 2.42129e-01
Current local training loss: 2.37809e-01
Current local training loss: 2.33110e-01
Current local training loss: 2.28356e-01
Current local training loss: 2.24061e-01
Current local training loss: 2.20506e-01
Current local training loss: 2.16534e-01
Current local training loss: 2.12897e-01
Current local training loss: 2.10148e-01
Current local training loss: 2.05965e-01
Current local training loss: 2.03486e-01
Current local training loss: 2.00976e-01
Current local training loss: 1.97900e-01
Current local training loss: 1.95704e-01
Current local training loss: 1.93631e-01
Current local training loss: 1.91045e-01
Current local training loss: 1.88809e-01
Current local training loss: 1.86326e-01
Current local training loss: 1.84116e-01
Current local training loss: 1.82143e-01
Current local training loss: 1.80021e-01
Current local training loss: 1.77776e-01
Current local training loss: 1.75960e-01
Current local training loss: 1.73907e-01
Current local training loss: 1.72177e-01
Current local training loss: 1.70417e-01
╭──────────────────────────────────────────────────────────────────────────────╮
│ 🚶  Training... step 5: train loss 8.2909       val loss 8.2272              │
╰──────────────────────────────────────────────────────────────────────────────╯wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         loss █▅▄▃▃▁▁▁▅▄▃▃▁▁▅▄▄▃▃▃▂▄▃▃▃▂▁▁▅▄▃▂▂▂▁▅▅▄▃▁
wandb:    step_loss ▁
wandb: step_time_ms ▁
wandb:   train_loss ▁▅▃▄▄█
wandb:     val_loss ▁▄█▃▄▆
wandb: 
wandb: Run summary:
wandb:         loss 0.17042
wandb:    step_loss 274.44658
wandb: step_time_ms 68326.51162
wandb:   train_loss 8.29095
wandb:     val_loss 8.22719
wandb: 
wandb: 🚀 View run fragrant-butterfly-33 at: https://wandb.ai/me322-university-of-heidelberg/MIDI-Transformer/runs/zgrhijx5
wandb: ⭐️ View project at: https://wandb.ai/me322-university-of-heidelberg/MIDI-Transformer
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250114_194830-zgrhijx5/logs
