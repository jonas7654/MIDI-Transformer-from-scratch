Running Training (with CUPY)
context length: 16
epochs: 32
batch size: 128
eval_interval: 1
learning rate: 0.1
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: me322 (me322-university-of-heidelberg). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /csghome/hpdc04/Transformer_Code/run_scripts/wandb/run-20250114_170345-4g53ol70
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-darkness-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/me322-university-of-heidelberg/MIDI-Transformer
wandb: üöÄ View run at https://wandb.ai/me322-university-of-heidelberg/MIDI-Transformer/runs/4g53ol70
Starting epoch: 1
Current local training loss: 3.02639e-01
Current local training loss: 2.75652e-01
Current local training loss: 2.72002e-01
Current local training loss: 2.69738e-01
Current local training loss: 2.68074e-01
Current local training loss: 2.66723e-01
Current local training loss: 2.65530e-01
Current local training loss: 2.64312e-01
Current local training loss: 2.63469e-01
Current local training loss: 2.62567e-01
Current local training loss: 2.61832e-01
Current local training loss: 2.60953e-01
Current local training loss: 2.60202e-01
Current local training loss: 2.59543e-01
Current local training loss: 2.58754e-01
Current local training loss: 2.58052e-01
Current local training loss: 2.57387e-01
Current local training loss: 2.56656e-01
Current local training loss: 2.55948e-01
Current local training loss: 2.55403e-01
Current local training loss: 2.54878e-01
Current local training loss: 2.54341e-01
Current local training loss: 2.53854e-01
Current local training loss: 2.53349e-01
Current local training loss: 2.52843e-01
Current local training loss: 2.52238e-01
Current local training loss: 2.51774e-01
Current local training loss: 2.51243e-01
Current local training loss: 2.50757e-01
Current local training loss: 2.50219e-01
Current local training loss: 2.49596e-01
Current local training loss: 2.49059e-01
Starting epoch: 2
Current local training loss: 2.76004e-01
Current local training loss: 2.75513e-01
Current local training loss: 2.75011e-01
Current local training loss: 2.74523e-01
Current local training loss: 2.74007e-01
Current local training loss: 2.73466e-01
Current local training loss: 2.73112e-01
Current local training loss: 2.72622e-01
Current local training loss: 2.72133e-01
Current local training loss: 2.71622e-01
Current local training loss: 2.71068e-01
Current local training loss: 2.70498e-01
Current local training loss: 2.69942e-01
Current local training loss: 2.69371e-01
Current local training loss: 2.68869e-01
Current local training loss: 2.68392e-01
Current local training loss: 2.67828e-01
Current local training loss: 2.67269e-01
Current local training loss: 2.66808e-01
Current local training loss: 2.66263e-01
Current local training loss: 2.65749e-01
Current local training loss: 2.65338e-01
Current local training loss: 2.64871e-01
Current local training loss: 2.64461e-01
Current local training loss: 2.63889e-01
Current local training loss: 2.63324e-01
Current local training loss: 2.62749e-01
Current local training loss: 2.62329e-01
Current local training loss: 2.61932e-01
Current local training loss: 2.61476e-01
Current local training loss: 2.61001e-01
Current local training loss: 2.60546e-01
